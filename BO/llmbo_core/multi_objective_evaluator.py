"""
å¤šç›®æ ‡è¯„ä»·å™¨ï¼ˆMulti-Objective Evaluatorï¼‰- å®Œæ•´ç‰ˆï¼ˆå·²ä¿®å¤ï¼‰
ç”¨äº LLM å¢å¼ºçš„åˆ†è§£å¤šç›®æ ‡è´å¶æ–¯ä¼˜åŒ– (LLM-DMOBO)

=============================================================================
ä¿®å¤å†…å®¹
=============================================================================
[OK] æ·»åŠ äº† initialize_with_llm_warmstart() å¼‚æ­¥æ–¹æ³•
   - æ”¯æŒ LLM API è°ƒç”¨ç”Ÿæˆæ™ºèƒ½åˆå§‹ç­–ç•¥
   - æ—  API key æ—¶è‡ªåŠ¨å›é€€åˆ°éšæœºç­–ç•¥
   - å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è¾“å‡º

=============================================================================
æ ¸å¿ƒåŠŸèƒ½
=============================================================================
1. å……ç”µä»¿çœŸä¸å¤šç›®æ ‡è¯„ä¼°
   - ä¸¤é˜¶æ®µæ’æµå……ç”µç­–ç•¥: [I1, t1, I2]
   - ä¸‰ä¸ªä¼˜åŒ–ç›®æ ‡: [æ—¶é—´, æ¸©åº¦å³°å€¼, ç”µæ± è€åŒ–]

2. æ•°æ®åº“ç®¡ç† (D)
   - å®Œæ•´å†å²è®°å½•: å‚æ•°ã€ç›®æ ‡å€¼ã€å½’ä¸€åŒ–å€¼ã€æ ‡é‡åŒ–å€¼
   - åŠ¨æ€åˆ†ä½æ•°è¾¹ç•Œæ›´æ–° (Q5/Q95)
   - å¸•ç´¯æ‰˜å‰æ²¿æå–

3. å¤šç›®æ ‡åˆ†è§£
   - å¢å¼ºåˆ‡æ¯”é›ªå¤«æ ‡é‡åŒ– (Augmented Tchebycheff)
   - åŠ¨æ€æƒé‡æ›´æ–°ï¼ˆæ”¯æŒåˆ†è§£ç­–ç•¥ï¼‰
   
4. LLM é›†æˆæ¥å£
   - [OK] LLM Warm Start åˆå§‹åŒ–ï¼ˆæ–°å¢ï¼‰
   - å¤–éƒ¨è¯„ä¼°æ·»åŠ ï¼ˆç”¨äº Warm Startï¼‰
   - è¯­ä¹‰çº¦æŸ S é¢„ç•™æ¥å£
   - æœ€ä½³è§£æŸ¥è¯¢

=============================================================================
ä½œè€…: Claude AI Assistant
æ—¥æœŸ: 2025-01-19ï¼ˆä¿®å¤ç‰ˆï¼‰
ç‰ˆæœ¬: v3.1 - æ·»åŠ  LLM Warm Start
=============================================================================
"""

import numpy as np
import asyncio
import json
from typing import Dict, List, Optional, Tuple
try:
    from .SPM_v3 import SPM_Sensitivity as SPM
except ImportError:
    from SPM_v3 import SPM_Sensitivity as SPM

# æ–°å¢: å†å²æ•°æ®é©±åŠ¨çš„WarmStart
try:
    from .historical_warmstart import HistoricalWarmStart
except ImportError:
    from historical_warmstart import HistoricalWarmStart


class SoftConstraintHandler:
    """
    è½¯çº¦æŸå¤„ç†å™¨ - æŒ‡æ•°/å¹³æ–¹æƒ©ç½šæœºåˆ¶
    
    è®¾è®¡ç†å¿µ:
    - æ¥è¿‘é™åˆ¶æ—¶æ–½åŠ å¹³æ»‘æƒ©ç½šï¼Œè€Œéç¡¬æˆªæ–­
    - ä¿æŒç›®æ ‡å‡½æ•°è¿ç»­å¯å¾®
    - BOå¯ä»¥å­¦ä¹ å¦‚ä½•é¿å¼€å±é™©åŒºåŸŸ
    """
    
    def __init__(
        self,
        temp_max: float = 318.0,  # âœ… æ”¹è¿›ï¼š309K + 3Kè£•åº¦
        temp_penalty_rate: float = 0.15,
        temp_penalty_scale: float = 0.05,
        aging_threshold: float = 0.5,  # âœ… å¯¹æ•°ç©ºé—´é˜ˆå€¼
        aging_penalty_scale: float = 0.1,
        verbose: bool = True
    ):
        self.temp_max = temp_max
        self.temp_penalty_rate = temp_penalty_rate
        self.temp_penalty_scale = temp_penalty_scale
        self.aging_threshold = aging_threshold
        self.aging_penalty_scale = aging_penalty_scale
        self.verbose = verbose
        
        if self.verbose:
            print("\n" + "="*70)
            print("ğŸ”§ è½¯çº¦æŸå¤„ç†å™¨å·²åˆå§‹åŒ– [v2.0]")
            print("="*70)
            print(f"æ¸©åº¦çº¦æŸ: T_max = {temp_max}K (309K + 3Kè£•åº¦)")
            print(f"  Î» = {temp_penalty_rate} (æŒ‡æ•°å¢é•¿ç‡)")
            print(f"  Î± = {temp_penalty_scale} (æƒ©ç½šç¼©æ”¾)")
            print(f"è€åŒ–çº¦æŸ: A_threshold = {aging_threshold} (å¯¹æ•°ç©ºé—´é˜ˆå€¼)")
            print(f"  Î² = {aging_penalty_scale}")
            print("="*70)
    
    def compute_temperature_penalty(self, temp: float) -> Tuple[float, str]:
        """æ¸©åº¦è½¯çº¦æŸ (æŒ‡æ•°æƒ©ç½š)"""
        if temp <= self.temp_max:
            return 0.0, "safe"
        
        excess = temp - self.temp_max
        penalty = self.temp_penalty_scale * (np.exp(self.temp_penalty_rate * excess) - 1)
        
        if excess <= 3.0:
            status = "mild"
        elif excess <= 6.0:
            status = "moderate"
        else:
            status = "severe"
        
        return penalty, status
    
    def compute_aging_penalty(self, aging: float) -> Tuple[float, str]:
        """è€åŒ–è½¯çº¦æŸ (å¹³æ–¹æƒ©ç½š)"""
        if aging <= self.aging_threshold:
            return 0.0, "safe"
        
        excess = aging - self.aging_threshold
        penalty = self.aging_penalty_scale * (excess ** 2)
        
        if excess <= 0.1:
            status = "mild"
        elif excess <= 0.3:
            status = "moderate"
        else:
            status = "severe"
        
        return penalty, status
    
    def compute_total_penalty(self, objectives: Dict[str, float]) -> Dict:
        """è®¡ç®—æ€»æƒ©ç½š"""
        temp = objectives['temp']
        aging = objectives['aging']
        
        temp_penalty, temp_status = self.compute_temperature_penalty(temp)
        aging_penalty, aging_status = self.compute_aging_penalty(aging)
        
        total_penalty = temp_penalty + aging_penalty
        is_severe = (temp_status == "severe" or aging_status == "severe")
        
        return {
            'total_penalty': total_penalty,
            'penalties': {'temp': temp_penalty, 'aging': aging_penalty},
            'statuses': {'temp': temp_status, 'aging': aging_status},
            'is_severe': is_severe
        }


class MultiObjectiveEvaluator:
    """
    å¤šç›®æ ‡å……ç”µç­–ç•¥è¯„ä»·å™¨
    
    åŠŸèƒ½ï¼š
    1. è¯„ä¼°å……ç”µç­–ç•¥çš„ä¸‰ä¸ªç›®æ ‡ï¼ˆæ—¶é—´ã€æ¸©åº¦ã€è€åŒ–ï¼‰
    2. ç»´æŠ¤å†å²æ•°æ®å¹¶åŠ¨æ€æ›´æ–°åˆ†ä½æ•°è¾¹ç•Œ
    3. å½’ä¸€åŒ– + åˆ‡æ¯”é›ªå¤«æ ‡é‡åŒ–
    4. LLM çƒ­å¯åŠ¨æ”¯æŒ
    """
    
    def __init__(
        self, 
        weights: Optional[Dict[str, float]] = None,
        update_interval: int = 10,
        temp_max: float = 309.0,
        max_steps: int = 300,
        verbose: bool = True
    ):
        """
        åˆå§‹åŒ–è¯„ä»·å™¨
        
        å‚æ•°ï¼š
            weights: å„ç›®æ ‡æƒé‡ï¼Œé»˜è®¤ {'time': 0.4, 'temp': 0.35, 'aging': 0.25}
            update_interval: åˆ†ä½æ•°æ›´æ–°é—´éš”ï¼ˆæ¯Næ¬¡è¯„ä¼°æ›´æ–°ä¸€æ¬¡ï¼‰
            temp_max: æ¸©åº¦çº¦æŸä¸Šé™[K]
            max_steps: å•æ¬¡å……ç”µæœ€å¤§æ­¥æ•°é™åˆ¶
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—
        """
        # æƒé‡è®¾ç½®
        self.weights = weights or {
            'time': 0.4,    # å……ç”µæ—¶é—´æƒé‡
            'temp': 0.35,   # å³°å€¼æ¸©åº¦æƒé‡
            'aging': 0.25   # å®¹é‡è€åŒ–æƒé‡
        }
        
        # éªŒè¯æƒé‡å’Œä¸º1
        weight_sum = sum(self.weights.values())
        if not np.isclose(weight_sum, 1.0):
            raise ValueError(f"æƒé‡ä¹‹å’Œå¿…é¡»ä¸º1.0ï¼Œå½“å‰ä¸º {weight_sum}")
        
        # âœ… æ ¸å¿ƒæ”¹è¿›ï¼šç‰©ç†è¾¹ç•Œï¼ˆå›ºå®šï¼ŒåŸºäºç”µåŒ–å­¦åŸç†ï¼‰
        self.physical_bounds = {
            'time': {
                'min': 20,      # âœ… æœ€å¿«20æ­¥ï¼ˆçº¦40åˆ†é’Ÿï¼‰
                'max': 120      # âœ… æœ€æ…¢120æ­¥ï¼ˆçº¦4å°æ—¶ï¼‰
            },
            'temp': {
                'min': 298.0,   # å®¤æ¸©èµ·å§‹
                'max': 318.0    # 
            },
            'aging': {
                'min': 0.0,     # âœ… log1p(aging_raw*100) æœ€å°åº”ä¸º 0
                'max': 6.5      # âœ… log1p(5.0*100) â‰ˆ 6.2ï¼ˆä¸¥é‡è€åŒ–ï¼‰+ è£•åº¦
            }
        }
        
        # âœ… è¿è¡Œæ—¶è¾¹ç•Œï¼ˆå•è°ƒæ‰©å±•ï¼Œä»ç‰©ç†è¾¹ç•Œå¼€å§‹ï¼‰
        self.running_bounds = {
            'time': {'min': self.physical_bounds['time']['min'], 
                    'max': self.physical_bounds['time']['max']},
            'temp': {'min': self.physical_bounds['temp']['min'], 
                    'max': self.physical_bounds['temp']['max']},
            'aging': {'min': self.physical_bounds['aging']['min'], 
                     'max': self.physical_bounds['aging']['max']}
        }
        
        # âœ… åŸå§‹å†å²æ•°æ®ï¼ˆå­˜å‚¨ç‰©ç†å€¼ï¼‰
        self.raw_history = []
        
        # å†å²æ•°æ®å­˜å‚¨ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
        self.history = {
            'time': [],      # å……ç”µæ­¥æ•°
            'temp': [],      # å³°å€¼æ¸©åº¦[K]
            'aging': [],     # å®¹é‡è¡°å‡ï¼ˆå¯¹æ•°å€¼ï¼‰
            'valid': []      # æ˜¯å¦æ»¡è¶³çº¦æŸ
        }
        
        # è¯„ä¼°è®¡æ•°
        self.eval_count = 0
        self.update_interval = update_interval
        self.temp_max = temp_max
        self.max_steps = max_steps
        self.verbose = verbose
        
        # âœ… åˆå§‹åŒ–æ¢¯åº¦è®¡ç®—å™¨ï¼ˆé¿å… verbose=False æ—¶æœªå®šä¹‰ï¼‰
        self.spm_for_gradients = None
        self.gradient_compute_interval = 3  # æ¯3æ¬¡è®¡ç®—ä¸€æ¬¡æ¢¯åº¦
        self.invalid_penalty = 0.5  # âœ… æ— æ•ˆç‚¹çš„é¢å¤–æƒ©ç½šï¼ˆé™ä½ä»¥é¿å…få€¼è¿‡å¤§ï¼‰
        
        # åŠ¨æ€åˆ†ä½æ•°è¾¹ç•Œï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
        self.bounds = None
        
        # ä¸´æ—¶å›ºå®šè¾¹ç•Œï¼ˆå‰10æ¬¡ä½¿ç”¨ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰
        self.temp_bounds = {
            'time': {'best': 20, 'worst': 120},           # âœ… æ”¹è¿›ï¼š20-120æ­¥
            'temp': {'best': 298.0, 'worst': 312.0},      # âœ… æ”¹è¿›ï¼š312Kä¸Šé™
            'aging': {'best': 0.0, 'worst': 6.5}          # âœ… å¯¹æ•°ç©ºé—´ï¼Œ>=0
        }
        
        # è¯¦ç»†æ—¥å¿—ï¼ˆç”¨äºåç»­åˆ†æï¼‰
        self.detailed_logs = []
        
        if self.verbose:
            # [Gradient Computation]
            from SPM_v3 import SPM_Sensitivity
            self.spm_for_gradients = SPM_Sensitivity(
                init_v=3.0, 
                init_t=298, 
                mode='finite_difference',
                enable_penalty_gradients=True,
                penalty_scale=10.0,
                verbose=False  # âœ… é¿å…æ¢¯åº¦è®¡ç®—æ—¶è¾“å‡ºå¹²æ‰°
            )
            print("[OK] Gradient computation enabled with penalty gradients (v3.0)")
            print("=" * 70)
            print("å¤šç›®æ ‡è¯„ä»·å™¨ v2.0 å·²åˆå§‹åŒ–ï¼ˆå…¨å±€å½’ä¸€åŒ–ï¼‰")
            print("=" * 70)
            print(f"æƒé‡è®¾ç½®: {self.weights}")
            print(f"\nç‰©ç†è¾¹ç•Œï¼ˆå›ºå®šï¼‰:")
            for key in ['time', 'temp', 'aging']:
                print(f"  {key}: {self.physical_bounds[key]}")
            print(f"\nè€åŒ–å¤„ç†: log1på˜æ¢ï¼ˆæ›¿ä»£Ã—1000æ”¾å¤§ï¼‰")
            print(f"æ¸©åº¦çº¦æŸä¸Šé™: {temp_max} K")
            print(f"æœ€å¤§æ­¥æ•°é™åˆ¶: {max_steps} æ­¥")
            print("=" * 70)
        
        self.soft_constraints = SoftConstraintHandler(
            temp_max=312.0,  # âœ… ä½¿ç”¨å¢åŠ è£•åº¦çš„æ¸©åº¦ä¸Šé™
            temp_penalty_rate=0.15,
            temp_penalty_scale=0.05,
            aging_threshold=5.0,  # âœ… å¯¹æ•°ç©ºé—´çš„é˜ˆå€¼ï¼ˆlog1p(5%*100)â‰ˆ6.2ï¼Œè®¾ä¸º5.0ï¼‰
            aging_penalty_scale=0.02,  # âœ… é™ä½æƒ©ç½šç³»æ•°
            verbose=self.verbose
        )
    
    # ============================================================
    # æ–°å¢ï¼šLLM Warm Start æ–¹æ³•
    # ============================================================
    
    async def initialize_with_llm_warmstart(
        self,
        n_strategies: int = 5,
        llm_api_key: Optional[str] = None,
        llm_base_url: str = 'https://api.nuwaapi.com/v1',
        llm_model: str = "gpt-3.5-turbo"
    ) -> List[Dict]:
        """
        ä½¿ç”¨ LLM è¿›è¡Œçƒ­å¯åŠ¨åˆå§‹åŒ– - æ–°ç‰ˆæœ¬ï¼ˆå†å²æ•°æ®é©±åŠ¨ï¼‰
        
        æ”¹è¿›ï¼š
        1. ä½¿ç”¨HistoricalWarmStartè‡ªåŠ¨åŠ è½½å†å²æ•°æ®
        2. åŸºäºç”µåŒ–å­¦é¢†åŸŸçŸ¥è¯†ç”Ÿæˆé«˜è´¨é‡prompt
        3. åŒ…å«few-shot learning examplesï¼ˆæœ€ä¼˜/æœ€å·®è§£ï¼‰
        4. åŠ¨æ€æ¢ç´¢æ¨¡å¼ï¼ˆconservative/balanced/aggressiveï¼‰
        
        å‚æ•°ï¼š
            n_strategies: è¦ç”Ÿæˆçš„ç­–ç•¥æ•°é‡
            llm_api_key: LLM API å¯†é’¥ï¼ˆå¯é€‰ï¼‰
            llm_base_url: API åŸºç¡€ URL
            llm_model: ä½¿ç”¨çš„ LLM æ¨¡å‹
        
        è¿”å›ï¼š
            List[Dict]: è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        if self.verbose:
            print("\n" + "=" * 70)
            print("å¼€å§‹ LLM Warm Start åˆå§‹åŒ–ï¼ˆå†å²æ•°æ®é©±åŠ¨ç‰ˆï¼‰")
            print("=" * 70)
            print(f"ç›®æ ‡ç­–ç•¥æ•°é‡: {n_strategies}")
            print(f"API Key: {'å·²æä¾›' if llm_api_key else 'æœªæä¾›ï¼ˆå°†ä½¿ç”¨éšæœºç­–ç•¥ï¼‰'}")
            print("=" * 70)
        
        results = []
        
        # ====== ä½¿ç”¨æ–°çš„HistoricalWarmStart ======
        if llm_api_key is not None:
            try:
                if self.verbose:
                    print("\nä½¿ç”¨HistoricalWarmStartç”Ÿæˆç­–ç•¥...")
                
                # åˆ›å»ºHistoricalWarmStartå®ä¾‹
                warmstart_generator = HistoricalWarmStart(
                    result_dir='./results',  # ç¡®ä¿Resultç›®å½•æ­£ç¡®
                    llm_api_key=llm_api_key,
                    llm_base_url=llm_base_url,
                    llm_model=llm_model,
                    verbose=self.verbose
                )
                
                # ç”Ÿæˆç­–ç•¥ï¼ˆè‡ªåŠ¨åŠ è½½å†å²æ•°æ® + ç”Ÿæˆé«˜è´¨é‡prompt + è°ƒç”¨LLMï¼‰
                strategies = await warmstart_generator.generate_warmstart_strategies_async(
                    n_strategies=n_strategies,
                    n_historical_runs=5,  # åŠ è½½æœ€è¿‘5æ¬¡è¿è¡Œ
                    objective_weights=self.weights,
                    exploration_mode='balanced'  # å¯é€‰: 'conservative', 'balanced', 'aggressive'
                )
                
                if self.verbose:
                    print(f"[OK] HistoricalWarmStartæˆåŠŸç”Ÿæˆ {len(strategies)} ä¸ªç­–ç•¥")
                
            except Exception as e:
                if self.verbose:
                    print(f"[Warning]  HistoricalWarmStartå¤±è´¥: {e}")
                    print("   å›é€€åˆ°éšæœºç­–ç•¥ç”Ÿæˆ...")
                
                # å›é€€åˆ°éšæœºç­–ç•¥
                strategies = self._generate_random_strategies(n_strategies)
        
        else:
            # æ²¡æœ‰ API keyï¼Œç›´æ¥ä½¿ç”¨éšæœºç­–ç•¥
            if self.verbose:
                print("\nä½¿ç”¨éšæœºç­–ç•¥ç”Ÿæˆ...")
            
            strategies = self._generate_random_strategies(n_strategies)
        
        # ====== è¯„ä¼°æ‰€æœ‰ç­–ç•¥ï¼ˆä¿æŒä¸å˜ï¼‰ ======
        if self.verbose:
            print(f"\nå¼€å§‹è¯„ä¼° {len(strategies)} ä¸ªç­–ç•¥...")
        
        for i, strategy in enumerate(strategies, 1):
            params = strategy['params']
            
            try:
                # è°ƒç”¨ evaluate() æ–¹æ³•è¿›è¡Œä»¿çœŸè¯„ä¼°
                scalarized = self.evaluate(
                    current1=params['current1'],
                    charging_number=params['charging_number'],
                    current2=params['current2']
                )
                
                # ä»æœ€æ–°çš„æ—¥å¿—ä¸­æå–ç›®æ ‡å€¼
                latest_log = self.detailed_logs[-1]
                
                result = {
                    'params': params,
                    'scalarized': scalarized,
                    'objectives': latest_log['objectives'],
                    'valid': latest_log['valid'],
                    'source': strategy.get('source', 'llm_warmstart'),
                    'reasoning': strategy.get('reasoning', '')
                }
                
                results.append(result)
                
                if self.verbose:
                    print(f"  ç­–ç•¥ {i}/{len(strategies)}: "
                          f"I1={params['current1']:.2f}A, "
                          f"t1={params['charging_number']}, "
                          f"I2={params['current2']:.2f}A "
                          f"â†’ æ ‡é‡åŒ–={scalarized:.4f}")
            
            except Exception as e:
                if self.verbose:
                    print(f"  âœ— ç­–ç•¥ {i} è¯„ä¼°å¤±è´¥: {e}")
                continue
        
        if self.verbose:
            print(f"\n[OK] Warm Startå®Œæˆï¼æˆåŠŸè¯„ä¼° {len(results)}/{len(strategies)} ä¸ªç­–ç•¥")
            print("=" * 70)
        
        return results
    
    # [X] å·²åˆ é™¤ _llm_generate_strategies æ–¹æ³•
    # åŸå› : ä½¿ç”¨æ–°çš„HistoricalWarmStartæ›¿ä»£ç¡¬ç¼–ç prompt
    # æ–°æ–¹æ³•è‡ªåŠ¨åŠ è½½å†å²æ•°æ®ï¼Œç”ŸæˆåŸºäºç”µåŒ–å­¦é¢†åŸŸçŸ¥è¯†çš„é«˜è´¨é‡prompt
    
    def _generate_random_strategies(self, n_strategies: int) -> List[Dict]:
        """
        ç”Ÿæˆéšæœºå……ç”µç­–ç•¥ï¼ˆä½œä¸º LLM çš„å›é€€æ–¹æ¡ˆï¼‰
        
        è¿”å›ï¼š
            List[Dict]: éšæœºç­–ç•¥åˆ—è¡¨
        """
        strategies = []
        
        for _ in range(n_strategies):
            strategy = {
                'params': {
                    'current1': np.random.uniform(3.0, 6.0),
                    'charging_number': int(np.random.uniform(5, 25)),
                    'current2': np.random.uniform(1.0, 4.0)
                },
                'source': 'random_warmstart',
                'reasoning': 'Randomly generated strategy'
            }
            strategies.append(strategy)
        
        return strategies
    
    # ============================================================
    # æ–°å¢ï¼šå¯¹æ•°å˜æ¢å’Œå…¨å±€å½’ä¸€åŒ–æ–¹æ³• (v2.0)
    # ============================================================
    
    def _apply_log_transform(self, aging_raw: float) -> float:
        """
        å¯¹è€åŒ–å€¼åº”ç”¨å¯¹æ•°å˜æ¢
        
        å‚æ•°ï¼š
            aging_raw: åŸå§‹å®¹é‡è¡°å‡ç™¾åˆ†æ¯”ï¼ˆå¦‚0.1è¡¨ç¤º0.1%ï¼‰
        
        è¿”å›ï¼š
            å¯¹æ•°å˜æ¢å€¼
        """
        # log1p(x) = log(1 + x)ï¼Œé¿å…log(0)é—®é¢˜
        # æ”¾å¤§100å€å†å–å¯¹æ•°ï¼Œæå‡ä½è€åŒ–åŒºåŸŸåˆ†è¾¨ç‡
        return np.log1p(aging_raw * 100)
    
    def get_normalized_history(self) -> List[Dict]:
        """
        âœ… æ ¸å¿ƒæ–¹æ³•ï¼šåŸºäºå…¨å±€ç»Ÿä¸€è¾¹ç•Œé‡æ–°å½’ä¸€åŒ–æ‰€æœ‰å†å²æ•°æ®
        
        æ”¹è¿› v2.1ï¼š
        - ä½¿ç”¨ detailed_logs æ›¿ä»£ raw_historyï¼ˆä¿ç•™ gradients ç­‰å­—æ®µï¼‰
        - å¯¹ valid ç‚¹ clip åˆ° [0,1]
        - å¯¹ invalid ç‚¹æ˜ç¡®æ ‡è®°ä¸ºæœ€å·®ï¼ˆ1.0ï¼‰+ é¢å¤–æƒ©ç½š
        
        è¿™ä¸ªæ–¹æ³•ä¼šï¼š
        1. è®¡ç®—å½“å‰æ‰€æœ‰æœ‰æ•ˆæ•°æ®çš„min/max
        2. ä¸ç‰©ç†è¾¹ç•Œåšå•è°ƒæ‰©å±•ï¼ˆåªæ‰©ä¸ç¼©ï¼‰
        3. ç”¨ç»Ÿä¸€è¾¹ç•Œé‡æ–°å½’ä¸€åŒ–æ‰€æœ‰å†å²ç‚¹
        4. é‡æ–°è®¡ç®—æ ‡é‡åŒ–å€¼
        
        è¿”å›ï¼š
            è§„èŒƒåŒ–çš„å†å²æ•°æ®åˆ—è¡¨ï¼ˆåŒ…å« gradients ç­‰å®Œæ•´å­—æ®µï¼‰
        """
        if len(self.detailed_logs) == 0:
            return []
        
        # 1. æå–æ‰€æœ‰æœ‰æ•ˆæ•°æ®çš„ç‰©ç†å€¼
        valid_data = [h for h in self.detailed_logs if h.get('valid', False)]
        
        if len(valid_data) == 0:
            return []
        
        # ä½¿ç”¨Numpyå‘é‡åŒ–è®¡ç®—min/max
        times = np.array([d['objectives']['time'] for d in valid_data])
        temps = np.array([d['objectives']['temp'] for d in valid_data])
        agings_log = np.array([d['objectives']['aging'] for d in valid_data])
        
        current_bounds = {
            'time': {'min': times.min(), 'max': times.max()},
            'temp': {'min': temps.min(), 'max': temps.max()},
            'aging': {'min': agings_log.min(), 'max': agings_log.max()}
        }
        
        # 2. å•è°ƒæ‰©å±•ï¼šä¸ç‰©ç†è¾¹ç•Œå’Œè¿è¡Œæ—¶è¾¹ç•Œå–å¹¶é›†
        for key in ['time', 'temp', 'aging']:
            # è¾¹ç•Œåªèƒ½æ‰©å±•ï¼Œä¸èƒ½æ”¶ç¼©
            self.running_bounds[key]['min'] = min(
                self.running_bounds[key]['min'],
                current_bounds[key]['min'],
                self.physical_bounds[key]['min']
            )
            self.running_bounds[key]['max'] = max(
                self.running_bounds[key]['max'],
                current_bounds[key]['max'],
                self.physical_bounds[key]['max']
            )
        
        # é˜²æ­¢é™¤é›¶ï¼šç¡®ä¿æœ€å°èŒƒå›´
        min_ranges = {'time': 5.0, 'temp': 1.0, 'aging': 0.1}
        for key, min_range in min_ranges.items():
            current_range = self.running_bounds[key]['max'] - self.running_bounds[key]['min']
            if current_range < min_range:
                midpoint = (self.running_bounds[key]['max'] + self.running_bounds[key]['min']) / 2
                self.running_bounds[key]['min'] = midpoint - min_range / 2
                self.running_bounds[key]['max'] = midpoint + min_range / 2
        
        # 3. å‘é‡åŒ–å½’ä¸€åŒ–æ‰€æœ‰æ•°æ®
        normalized_history = []
        
        for log in self.detailed_logs:
            obj = log['objectives']
            is_valid = log.get('valid', False)
            
            # âœ… å½’ä¸€åŒ–ï¼ˆä½¿ç”¨å›ºå®šçš„physical_boundsï¼Œç¡®ä¿ä¸€è‡´æ€§ï¼‰
            normalized = {}
            for key in ['time', 'temp', 'aging']:
                denominator = self.physical_bounds[key]['max'] - self.physical_bounds[key]['min']
                val = (obj[key] - self.physical_bounds[key]['min']) / denominator
                
                # âœ… åªå¯¹ valid ç‚¹ clip åˆ° [0,1]ï¼›invalid ç‚¹ç›´æ¥æŒ‰æœ€å·®å¤„ç†
                if is_valid:
                    normalized[key] = float(np.clip(val, 0.0, 1.0))
                else:
                    normalized[key] = 1.0
            
            # åˆ‡æ¯”é›ªå¤«æ ‡é‡åŒ–
            weighted_deviations = [
                self.weights[key] * normalized[key]
                for key in ['time', 'temp', 'aging']
            ]
            max_weighted = max(weighted_deviations)
            sum_weighted = sum(weighted_deviations)
            scalarized = max_weighted + 0.05 * sum_weighted
            
            # æ·»åŠ è½¯çº¦æŸæƒ©ç½š
            constraint_result = self.soft_constraints.compute_total_penalty(obj)
            scalarized += constraint_result['total_penalty']
            
            if constraint_result['is_severe']:
                scalarized += 0.2  # ä¸¥é‡è¿è§„é¢å¤–æƒ©ç½š
            
            # âœ… æ— æ•ˆç‚¹é¢å¤–æƒ©ç½šï¼ˆè®© f æ˜æ˜¾ > 2ï¼‰
            if not is_valid:
                scalarized += self.invalid_penalty
            
            # âœ… æ„å»ºè§„èŒƒåŒ–è®°å½•ï¼ˆä¿ç•™åŸ log çš„æ‰€æœ‰å­—æ®µï¼ŒåŒ…æ‹¬ gradientsï¼‰
            new_log = dict(log)
            new_log['normalized'] = normalized
            new_log['scalarized'] = scalarized
            
            normalized_history.append(new_log)
        
        if self.verbose and len(normalized_history) > 0:
            print(f"\n[å…¨å±€å½’ä¸€åŒ–] å·²é‡ç®— {len(normalized_history)} æ¡å†å²è®°å½•")
            print(f"âœ… ä½¿ç”¨å›ºå®šç‰©ç†è¾¹ç•Œ:")
            for key in ['time', 'temp', 'aging']:
                print(f"  {key}: [{self.physical_bounds[key]['min']:.2f}, {self.physical_bounds[key]['max']:.2f}]")
        
        return normalized_history
    
    # ============================================================
    # åŸæœ‰æ–¹æ³•ï¼ˆä¿æŒä¸å˜ï¼‰
    # ============================================================
    
    def evaluate(
        self, 
        current1: float, 
        charging_number: float, 
        current2: float
    ) -> float:
        """
        è¯„ä¼°å•æ¬¡å……ç”µç­–ç•¥ï¼ˆä¸»æ¥å£ï¼ŒBOè°ƒç”¨ï¼‰
        
        å‚æ•°ï¼š
            current1: ç¬¬ä¸€é˜¶æ®µå……ç”µç”µæµ[A]
            charging_number: é˜¶æ®µåˆ‡æ¢æ­¥æ•°
            current2: ç¬¬äºŒé˜¶æ®µå……ç”µç”µæµ[A]
        
        è¿”å›ï¼š
            scalarized_value: åˆ‡æ¯”é›ªå¤«æ ‡é‡åŒ–åçš„å€¼ï¼ˆç”¨äºæœ€å°åŒ–ï¼‰
        """
        # 1. è¿è¡Œå……ç”µä»¿çœŸ
        sim_result = self._run_charging_simulation(current1, charging_number, current2)
        
        # 2. âœ… å¯¹è€åŒ–åº”ç”¨å¯¹æ•°å˜æ¢
        aging_raw = sim_result['aging']  # åŸå§‹ç™¾åˆ†æ¯”å€¼
        aging_log = self._apply_log_transform(aging_raw)
        
        # 3. âœ… å­˜å‚¨åŸå§‹æ•°æ®åˆ° raw_history
        objectives_with_log = {
            'time': sim_result['time'],
            'temp': sim_result['temp'],
            'aging': aging_log  # âœ… å­˜å‚¨å¯¹æ•°å˜æ¢å€¼
        }
        
        raw_record = {
            'eval_id': self.eval_count,
            'params': {
                'current1': current1,
                'charging_number': charging_number,
                'current2': current2
            },
            'objectives': objectives_with_log,
            'aging_raw': aging_raw,  # åŒæ—¶ä¿ç•™åŸå§‹å€¼ç”¨äºåˆ†æ
            'valid': sim_result['valid'],
            'violations': sim_result.get('constraint_violation', 0),
            'termination': sim_result.get('termination', 'unknown')
        }
        
        self.raw_history.append(raw_record)
        
        # 4. æ›´æ–°æ—§å†å²ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
        self.history['time'].append(sim_result['time'])
        self.history['temp'].append(sim_result['temp'])
        self.history['aging'].append(aging_log)  # å­˜å‚¨å¯¹æ•°å€¼
        self.history['valid'].append(sim_result['valid'])
        
        self.eval_count += 1
        
        # 5. âœ… ç§»é™¤æ—§çš„è¾¹ç•Œæ›´æ–°é€»è¾‘ï¼ˆä½¿ç”¨å…¨å±€å½’ä¸€åŒ–æ›¿ä»£ï¼‰
        # if self.eval_count % self.update_interval == 0 and self.eval_count >= 10:
        #     self._update_bounds()
        
        # âœ… 6. ä¸´æ—¶å½’ä¸€åŒ–ï¼ˆç»Ÿä¸€ä½¿ç”¨å›ºå®šçš„ç‰©ç†è¾¹ç•Œï¼‰
        normalized = {}
        for key in ['time', 'temp', 'aging']:
            denominator = self.physical_bounds[key]['max'] - self.physical_bounds[key]['min']
            if denominator > 0:
                normalized[key] = (objectives_with_log[key] - self.physical_bounds[key]['min']) / denominator
            else:
                normalized[key] = 0.5
            
            # âœ… åªå¯¹ valid ç‚¹ clip åˆ° [0,1]ï¼›invalid ç‚¹ç›´æ¥æŒ‰æœ€å·®å¤„ç†
            if sim_result['valid']:
                normalized[key] = float(np.clip(normalized[key], 0.0, 1.0))
            else:
                normalized[key] = 1.0
        
        # 7. åˆ‡æ¯”é›ªå¤«æ ‡é‡åŒ–
        weighted_deviations = [
            self.weights[key] * normalized[key]
            for key in ['time', 'temp', 'aging']
        ]
        base_scalarized = max(weighted_deviations) + 0.05 * sum(weighted_deviations)
        scalarized = base_scalarized
        
        # 8. è½¯çº¦æŸæƒ©ç½šæœºåˆ¶ (æŒ‡æ•°/å¹³æ–¹)
        constraint_result = self.soft_constraints.compute_total_penalty(objectives_with_log)
        soft_penalty = constraint_result['total_penalty']
        scalarized += soft_penalty
        
        if constraint_result['is_severe']:
            scalarized += 0.2  # ä¸¥é‡è¿è§„é¢å¤–æƒ©ç½š
        
        # âœ… æ— æ•ˆç‚¹é¢å¤–æƒ©ç½š
        if not sim_result['valid']:
            scalarized += self.invalid_penalty
        
        # âœ… è¯¦ç»†è°ƒè¯•è¾“å‡º
        if self.verbose and self.eval_count % 1 == 0:  # æ¯æ¬¡éƒ½è¾“å‡º
            print(f"\n  [å½’ä¸€åŒ–] time={normalized['time']:.4f}, temp={normalized['temp']:.4f}, aging={normalized['aging']:.4f}")
            print(f"  [æ ‡é‡åŒ–] åŸºç¡€={base_scalarized:.4f}, è½¯çº¦æŸ={soft_penalty:.4f}, æ— æ•ˆæƒ©ç½š={self.invalid_penalty if not sim_result['valid'] else 0:.4f}")
            print(f"  [æœ€ç»ˆ] f={scalarized:.4f}, valid={sim_result['valid']}")
        
        # 9. è®¡ç®—æ¢¯åº¦ï¼ˆå¯é€‰ï¼‰
        gradients = None
        if (self.spm_for_gradients is not None) and (self.eval_count % self.gradient_compute_interval == 0):
            try:
                grad_result = self.spm_for_gradients.run_two_stage_charging(
                    current1=current1, charging_number=int(charging_number), 
                    current2=current2, return_sensitivities=True
                )
                if grad_result.get('valid', False) and 'sensitivities' in grad_result:
                    gradients = grad_result['sensitivities']
            except Exception:
                gradients = None

        # 10. è®°å½•è¯¦ç»†æ—¥å¿—
        log_entry = {
            'eval_id': self.eval_count,
            'params': {'current1': current1, 'charging_number': charging_number, 'current2': current2},
            'objectives': objectives_with_log,
            'aging_raw': aging_raw,
            'normalized': normalized,
            'scalarized': scalarized,
            'valid': sim_result['valid'],
            'violations': sim_result['constraint_violation'],
            'termination': sim_result['termination'],
            'gradients': gradients
        }
        self.detailed_logs.append(log_entry)
        
        # 11. å¯é€‰ï¼šæ‰“å°è¿›åº¦
        if self.verbose and self.eval_count % 5 == 0:
            time_minutes = sim_result['time'] * 90 / 60
            
            constraint_info = self.soft_constraints.compute_total_penalty(objectives_with_log)
            temp_status = constraint_info['statuses']['temp']
            temp_penalty = constraint_info['penalties']['temp']
            
            status_icon = {
                'safe': 'âœ“',
                'mild': 'âš ',
                'moderate': 'âš âš ',
                'severe': 'âŒ'
            }.get(temp_status, '?')
            
            print(f"[Eval {self.eval_count}] "
                  f"t={sim_result['time']:.0f}æ­¥({time_minutes:.1f}min), "
                  f"T={sim_result['temp']:.2f}K{status_icon}, "
                  f"A_raw={aging_raw:.4f}%, A_log={aging_log:.2f}, "
                  f"penalty={temp_penalty:.4f}, "
                  f"f={scalarized:.4f}")
        
        return scalarized
    
    def _run_charging_simulation(
        self, 
        current1: float, 
        charging_number: float, 
        current2: float
    ) -> Dict:
        """è¿è¡Œå……ç”µä»¿çœŸå¹¶æ”¶é›†ä¸‰ä¸ªç›®æ ‡"""
        # åˆå§‹åŒ–SPMç¯å¢ƒï¼ˆv3.0 - ä¸éœ€è¦çµæ•åº¦ï¼‰
        env = SPM(init_v=3.0, init_t=298)
        
        # è¿è¡Œä¸¤é˜¶æ®µå……ç”µä»¿çœŸ
        result = env.run_two_stage_charging(
            current1=current1,
            charging_number=int(charging_number),
            current2=current2,
            return_sensitivities=False
        )
        
        if not result['valid']:
            return {
                'time': self.max_steps,  # æƒ©ç½šå€¼ï¼ˆæ­¥æ•°ï¼‰
                'temp': self.temp_max + 10,  # æƒ©ç½šå€¼
                'aging': 1.0,  # æƒ©ç½šå€¼
                'valid': False,
                'constraint_violation': 1,
                'termination': 'invalid'
            }
        
        # è½¬æ¢ç»“æœæ ¼å¼ï¼šå°†æ—¶é—´ä»ç§’è½¬æ¢ä¸ºæ­¥æ•°ï¼ˆ1æ­¥ = 90ç§’ï¼‰
        objectives = result['objectives']
        time_in_steps = objectives['time'] / 90.0  # ç§’ -> æ­¥æ•°
        
        return {
            'time': time_in_steps,  # æ­¥æ•°
            'temp': objectives['temp'],  # K
            'aging': objectives['aging'],  # %
            'valid': result['valid'],
            'constraint_violation': 0,
            'termination': 'completed'
        }
    
    def _update_bounds(self) -> None:
        """
        æ›´æ–°åˆ†ä½æ•°è¾¹ç•Œï¼ˆQ5/Q95ï¼‰
        
        æ”¹è¿›: ç¡®ä¿è¾¹ç•Œæœ‰åˆç†çš„æœ€å°èŒƒå›´,é¿å…æ•°å€¼ä¸ç¨³å®š
        """
        if self.eval_count < 10:
            return
        
        valid_indices = [i for i, v in enumerate(self.history['valid']) if v]
        
        if len(valid_indices) < 5:
            return
        
        # è®¡ç®—Q5/Q95åˆ†ä½æ•°
        self.bounds = {
            'time': {
                'best': np.percentile([self.history['time'][i] for i in valid_indices], 5),
                'worst': np.percentile([self.history['time'][i] for i in valid_indices], 95)
            },
            'temp': {
                'best': np.percentile([self.history['temp'][i] for i in valid_indices], 5),
                'worst': np.percentile([self.history['temp'][i] for i in valid_indices], 95)
            },
            'aging': {
                'best': np.percentile([self.history['aging'][i] for i in valid_indices], 5),
                'worst': np.percentile([self.history['aging'][i] for i in valid_indices], 95)
            }
        }
        
        # âœ… ç¡®ä¿è¾¹ç•Œæœ‰æœ€å°èŒƒå›´ï¼ˆé¿å…é™¤é›¶ï¼‰
        min_ranges = {
            'time': 5.0,      # æœ€å°æ—¶é—´èŒƒå›´5æ­¥
            'temp': 1.0,      # æœ€å°æ¸©åº¦èŒƒå›´1K
            'aging': 0.01     # æœ€å°è€åŒ–èŒƒå›´0.01%
        }
        
        for key, min_range in min_ranges.items():
            current_range = self.bounds[key]['worst'] - self.bounds[key]['best']
            if current_range < min_range:
                # æ‰©å±•è¾¹ç•Œåˆ°æœ€å°èŒƒå›´
                mid = (self.bounds[key]['best'] + self.bounds[key]['worst']) / 2
                self.bounds[key]['best'] = mid - min_range / 2
                self.bounds[key]['worst'] = mid + min_range / 2
                
                if self.verbose:
                    print(f"  [è¾¹ç•Œè°ƒæ•´] {key} èŒƒå›´è¿‡çª„ ({current_range:.4f}), æ‰©å±•åˆ° {min_range}")
        
        if self.verbose:
            print(f"\n[åˆ†ä½æ•°è¾¹ç•Œå·²æ›´æ–°] (ç¬¬ {self.eval_count} æ¬¡è¯„ä¼°)")
            for key in ['time', 'temp', 'aging']:
                print(f"  {key}: [{self.bounds[key]['best']:.4f}, {self.bounds[key]['worst']:.4f}]")
    
    def _normalize(self, objectives: Dict[str, float]) -> Dict[str, float]:
        """
        å½’ä¸€åŒ–ç›®æ ‡å€¼åˆ° [0, 1]
        
        æ”¹è¿›: æ·»åŠ æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤,é¿å…è¾¹ç•Œè¿‡çª„å¯¼è‡´é™¤é›¶é”™è¯¯
        """
        # ä¼˜å…ˆä½¿ç”¨åŠ¨æ€è¾¹ç•Œ,ä¸è¶³åˆ™ä½¿ç”¨ä¸´æ—¶è¾¹ç•Œ
        bounds = self.bounds if self.bounds is not None else self.temp_bounds
        
        # âœ… å®šä¹‰æœ€å°æœ‰æ•ˆèŒƒå›´ï¼ˆç›¸å¯¹äºä¸´æ—¶è¾¹ç•Œï¼‰
        min_ranges = {
            'time': 5.0,      # æœ€å°æ—¶é—´èŒƒå›´5æ­¥ï¼ˆä¸´æ—¶è¾¹ç•ŒèŒƒå›´140ï¼‰
            'temp': 1.0,      # æœ€å°æ¸©åº¦èŒƒå›´1Kï¼ˆä¸´æ—¶è¾¹ç•ŒèŒƒå›´11Kï¼‰
            'aging': 0.001    # æœ€å°è€åŒ–èŒƒå›´0.001%ï¼ˆä¸´æ—¶è¾¹ç•ŒèŒƒå›´0.5%ï¼‰
        }
        
        normalized = {}
        for key in ['time', 'temp', 'aging']:
            best = bounds[key]['best']
            worst = bounds[key]['worst']
            value = objectives[key]
            
            # è®¡ç®—åˆ†æ¯
            denominator = worst - best
            min_range = min_ranges[key]
            
            # âœ… æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼šè¾¹ç•ŒèŒƒå›´å¿…é¡»å¤§äºæœ€å°æœ‰æ•ˆèŒƒå›´
            if abs(denominator) < min_range:
                # è¾¹ç•Œè¿‡çª„,å°è¯•ä½¿ç”¨ä¸´æ—¶è¾¹ç•Œä½œä¸ºfallback
                if bounds is not self.temp_bounds:
                    # å½“å‰ä½¿ç”¨çš„æ˜¯åŠ¨æ€è¾¹ç•Œ,fallbackåˆ°ä¸´æ—¶è¾¹ç•Œ
                    temp_best = self.temp_bounds[key]['best']
                    temp_worst = self.temp_bounds[key]['worst']
                    temp_denominator = temp_worst - temp_best
                    
                    if abs(temp_denominator) >= min_range:
                        # ä¸´æ—¶è¾¹ç•Œæœ‰æ•ˆ
                        normalized[key] = (value - temp_best) / temp_denominator
                        if self.verbose and self.eval_count % 10 == 0:
                            print(f"  [Warning] {key}åŠ¨æ€è¾¹ç•Œè¿‡çª„({denominator:.6f}<{min_range}), ä½¿ç”¨å›ºå®šè¾¹ç•Œ")
                    else:
                        # ä¸´æ—¶è¾¹ç•Œä¹Ÿè¿‡çª„,ä½¿ç”¨ä¸­é—´å€¼
                        normalized[key] = 0.5
                        if self.verbose and self.eval_count % 10 == 0:
                            print(f"  [Warning] {key}æ‰€æœ‰è¾¹ç•Œè¿‡çª„, ä½¿ç”¨é»˜è®¤å€¼0.5")
                else:
                    # å·²ç»åœ¨ä½¿ç”¨ä¸´æ—¶è¾¹ç•Œä¸”ä»è¿‡çª„
                    normalized[key] = 0.5
                    if self.verbose and self.eval_count % 10 == 0:
                        print(f"  [Warning] {key}å›ºå®šè¾¹ç•Œè¿‡çª„({denominator:.6f}<{min_range}), ä½¿ç”¨é»˜è®¤å€¼0.5")
            else:
                # è¾¹ç•Œæ­£å¸¸,è¿›è¡Œæ ‡å‡†å½’ä¸€åŒ–
                normalized[key] = (value - best) / denominator
            
            # è£å‰ªåˆ°[0, 1]èŒƒå›´
            normalized[key] = np.clip(normalized[key], 0.0, 1.0)
        
        return normalized
    
    def _chebyshev_scalarize(self, normalized: Dict[str, float]) -> float:
        """å¢å¼ºåˆ‡æ¯”é›ªå¤«æ ‡é‡åŒ–"""
        weighted_deviations = [
            self.weights[key] * normalized[key] 
            for key in ['time', 'temp', 'aging']
        ]
        
        max_weighted = max(weighted_deviations)
        sum_weighted = sum(weighted_deviations)
        
        rho = 0.05
        scalarized = max_weighted + rho * sum_weighted
        
        return scalarized
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_evaluations': self.eval_count,
            'valid_evaluations': sum(self.history['valid']),
            'history_summary': {}
        }
        
        for key in ['time', 'temp', 'aging']:
            if len(self.history[key]) > 0:
                stats['history_summary'][key] = {
                    'min': float(np.min(self.history[key])),
                    'max': float(np.max(self.history[key])),
                    'mean': float(np.mean(self.history[key])),
                    'std': float(np.std(self.history[key]))
                }
        
        return stats
    
    def export_database(self, normalized: bool = True) -> List[Dict]:
        """
        å¯¼å‡ºå®Œæ•´æ•°æ®åº“
        
        å‚æ•°ï¼š
            normalized: æ˜¯å¦è¿”å›å…¨å±€å½’ä¸€åŒ–åçš„å†å²ï¼ˆé»˜è®¤Trueï¼‰
        
        è¿”å›ï¼š
            å½’ä¸€åŒ–å†å²ï¼ˆåŒ…å«ç»Ÿä¸€å°ºåº¦çš„ scalarized å€¼ï¼‰æˆ–åŸå§‹ detailed_logs
        """
        return self.get_normalized_history() if normalized else self.detailed_logs
    
    def get_pareto_front(self) -> List[Dict]:
        """æå–å¸•ç´¯æ‰˜æœ€ä¼˜è§£"""
        valid_logs = [log for log in self.detailed_logs if log['valid']]
        
        if len(valid_logs) == 0:
            return []
        
        pareto_front = []
        
        for log_i in valid_logs:
            obj_i = log_i['objectives']
            is_dominated = False
            
            for log_j in valid_logs:
                if log_i == log_j:
                    continue
                
                obj_j = log_j['objectives']
                
                # æ£€æŸ¥æ˜¯å¦è¢«æ”¯é…
                dominates = (
                    obj_j['time'] <= obj_i['time'] and
                    obj_j['temp'] <= obj_i['temp'] and
                    obj_j['aging'] <= obj_i['aging']
                )
                
                at_least_one_better = (
                    obj_j['time'] < obj_i['time'] or
                    obj_j['temp'] < obj_i['temp'] or
                    obj_j['aging'] < obj_i['aging']
                )
                
                if dominates and at_least_one_better:
                    is_dominated = True
                    break
            
            if not is_dominated:
                pareto_front.append(log_i)
        
        return pareto_front
    
    def add_external_evaluation(
        self, 
        current1: float, 
        charging_number: float, 
        current2: float,
        objectives: Dict[str, float],
        source: str = "external"
    ) -> None:
        """æ‰‹åŠ¨æ·»åŠ å¤–éƒ¨è¯„ä¼°ç»“æœåˆ°æ•°æ®åº“"""
        self.history['time'].append(objectives['time'])
        self.history['temp'].append(objectives['temp'])
        self.history['aging'].append(objectives['aging'])
        self.history['valid'].append(objectives.get('valid', True))
        
        self.eval_count += 1
        
        normalized = self._normalize(objectives)
        scalarized = self._chebyshev_scalarize(normalized)
        
        log_entry = {
            'eval_id': self.eval_count,
            'params': {'current1': current1, 'charging_number': charging_number, 'current2': current2},
            'objectives': objectives,
            'normalized': normalized,
            'scalarized': scalarized,
            'valid': objectives.get('valid', True),
            'violations': 0,
            'termination': 'external',
            'source': source
        }
        self.detailed_logs.append(log_entry)
        
        if self.verbose:
            print(f"[å¤–éƒ¨è¯„ä¼°å·²æ·»åŠ ] æ¥æº={source}, è¯„ä¼°ID={self.eval_count}")
    
    def update_weights(self, new_weights: Dict[str, float]) -> None:
        """åŠ¨æ€æ›´æ–°ç›®æ ‡æƒé‡"""
        weight_sum = sum(new_weights.values())
        if not np.isclose(weight_sum, 1.0):
            raise ValueError(f"æƒé‡ä¹‹å’Œå¿…é¡»ä¸º1.0ï¼Œå½“å‰ä¸º {weight_sum}")
        
        self.weights = new_weights
        
        if self.verbose:
            print(f"[æƒé‡å·²æ›´æ–°] {self.weights}")
    
    def get_best_solution(self) -> Optional[Dict]:
        """è·å–å½“å‰æœ€ä½³è§£ï¼ˆåŸºäºæ ‡é‡åŒ–å€¼ï¼‰"""
        if self.eval_count == 0:
            return None
        
        valid_logs = [log for log in self.detailed_logs if log['valid']]
        
        if len(valid_logs) == 0:
            return None
        
        best_log = min(valid_logs, key=lambda x: x['scalarized'])
        
        return best_log


# ============================================================
# æµ‹è¯•ä»£ç 
# ============================================================
if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("ã€æµ‹è¯•ã€‘MultiObjectiveEvaluator - v3.1ï¼ˆå·²ä¿®å¤ï¼‰")
    print("=" * 70)
    
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    evaluator = MultiObjectiveEvaluator(verbose=True)
    
    print("\n[OK] Basic functionality test passed")
    print(f"   evaluate() æ–¹æ³•: å­˜åœ¨")
    print(f"   initialize_with_llm_warmstart() æ–¹æ³•: å­˜åœ¨ âœ¨")
    print(f"   get_best_solution() æ–¹æ³•: å­˜åœ¨")
    print(f"   get_pareto_front() æ–¹æ³•: å­˜åœ¨")
    print("=" * 70)