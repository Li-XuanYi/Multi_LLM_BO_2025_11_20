"""
å¤šç›®æ ‡è¯„ä»·å™¨ï¼ˆMulti-Objective Evaluatorï¼‰- å®Œæ•´ç‰ˆï¼ˆå·²ä¿®å¤ï¼‰
ç”¨äº LLM å¢å¼ºçš„åˆ†è§£å¤šç›®æ ‡è´å¶æ–¯ä¼˜åŒ– (LLM-DMOBO)

=============================================================================
ä¿®å¤å†…å®¹
=============================================================================
[OK] æ·»åŠ äº† initialize_with_llm_warmstart() å¼‚æ­¥æ–¹æ³•
   - æ”¯æŒ LLM API è°ƒç”¨ç”Ÿæˆæ™ºèƒ½åˆå§‹ç­–ç•¥
   - æ—  API key æ—¶è‡ªåŠ¨å›é€€åˆ°éšæœºç­–ç•¥
   - å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è¾“å‡º

=============================================================================
æ ¸å¿ƒåŠŸèƒ½
=============================================================================
1. å……ç”µä»¿çœŸä¸å¤šç›®æ ‡è¯„ä¼°
   - ä¸¤é˜¶æ®µæ’æµå……ç”µç­–ç•¥: [I1, t1, I2]
   - ä¸‰ä¸ªä¼˜åŒ–ç›®æ ‡: [æ—¶é—´, æ¸©åº¦å³°å€¼, ç”µæ± è€åŒ–]

2. æ•°æ®åº“ç®¡ç† (D)
   - å®Œæ•´å†å²è®°å½•: å‚æ•°ã€ç›®æ ‡å€¼ã€å½’ä¸€åŒ–å€¼ã€æ ‡é‡åŒ–å€¼
   - åŠ¨æ€åˆ†ä½æ•°è¾¹ç•Œæ›´æ–° (Q5/Q95)
   - å¸•ç´¯æ‰˜å‰æ²¿æå–

3. å¤šç›®æ ‡åˆ†è§£
   - å¢å¼ºåˆ‡æ¯”é›ªå¤«æ ‡é‡åŒ– (Augmented Tchebycheff)
   - åŠ¨æ€æƒé‡æ›´æ–°ï¼ˆæ”¯æŒåˆ†è§£ç­–ç•¥ï¼‰
   
4. LLM é›†æˆæ¥å£
   - [OK] LLM Warm Start åˆå§‹åŒ–ï¼ˆæ–°å¢ï¼‰
   - å¤–éƒ¨è¯„ä¼°æ·»åŠ ï¼ˆç”¨äº Warm Startï¼‰
   - è¯­ä¹‰çº¦æŸ S é¢„ç•™æ¥å£
   - æœ€ä½³è§£æŸ¥è¯¢

=============================================================================
ä½œè€…: Claude AI Assistant
æ—¥æœŸ: 2025-01-19ï¼ˆä¿®å¤ç‰ˆï¼‰
ç‰ˆæœ¬: v3.1 - æ·»åŠ  LLM Warm Start
=============================================================================
"""

import numpy as np
import asyncio
import json
from typing import Dict, List, Optional, Tuple
try:
    from .SPM_v3 import SPM_Sensitivity as SPM
except ImportError:
    from SPM_v3 import SPM_Sensitivity as SPM

# æ–°å¢: å†å²æ•°æ®é©±åŠ¨çš„WarmStart
try:
    from .historical_warmstart import HistoricalWarmStart
except ImportError:
    from historical_warmstart import HistoricalWarmStart


class SoftConstraintHandler:
    """
    è½¯çº¦æŸå¤„ç†å™¨ - æŒ‡æ•°/å¹³æ–¹æƒ©ç½šæœºåˆ¶
    
    è®¾è®¡ç†å¿µ:
    - æ¥è¿‘é™åˆ¶æ—¶æ–½åŠ å¹³æ»‘æƒ©ç½šï¼Œè€Œéç¡¬æˆªæ–­
    - ä¿æŒç›®æ ‡å‡½æ•°è¿ç»­å¯å¾®
    - BOå¯ä»¥å­¦ä¹ å¦‚ä½•é¿å¼€å±é™©åŒºåŸŸ
    """
    
    def __init__(
        self,
        temp_max: float = 315.0,
        temp_penalty_rate: float = 0.15,
        temp_penalty_scale: float = 0.05,
        aging_threshold: float = 0.3,
        aging_penalty_scale: float = 0.1,
        verbose: bool = True
    ):
        self.temp_max = temp_max
        self.temp_penalty_rate = temp_penalty_rate
        self.temp_penalty_scale = temp_penalty_scale
        self.aging_threshold = aging_threshold
        self.aging_penalty_scale = aging_penalty_scale
        self.verbose = verbose
        
        if self.verbose:
            print("\n" + "="*70)
            print("ğŸ”§ è½¯çº¦æŸå¤„ç†å™¨å·²åˆå§‹åŒ–")
            print("="*70)
            print(f"æ¸©åº¦çº¦æŸ: T_max = {temp_max}K (æŒ‡æ•°æƒ©ç½š)")
            print(f"  Î» = {temp_penalty_rate} (å¢é•¿ç‡)")
            print(f"  Î± = {temp_penalty_scale} (ç¼©æ”¾)")
            print(f"è€åŒ–çº¦æŸ: A_threshold = {aging_threshold}% (å¹³æ–¹æƒ©ç½š)")
            print(f"  Î² = {aging_penalty_scale}")
            print("="*70)
    
    def compute_temperature_penalty(self, temp: float) -> Tuple[float, str]:
        """æ¸©åº¦è½¯çº¦æŸ (æŒ‡æ•°æƒ©ç½š)"""
        if temp <= self.temp_max:
            return 0.0, "safe"
        
        excess = temp - self.temp_max
        penalty = self.temp_penalty_scale * (np.exp(self.temp_penalty_rate * excess) - 1)
        
        if excess <= 3.0:
            status = "mild"
        elif excess <= 6.0:
            status = "moderate"
        else:
            status = "severe"
        
        return penalty, status
    
    def compute_aging_penalty(self, aging: float) -> Tuple[float, str]:
        """è€åŒ–è½¯çº¦æŸ (å¹³æ–¹æƒ©ç½š)"""
        if aging <= self.aging_threshold:
            return 0.0, "safe"
        
        excess = aging - self.aging_threshold
        penalty = self.aging_penalty_scale * (excess ** 2)
        
        if excess <= 0.1:
            status = "mild"
        elif excess <= 0.3:
            status = "moderate"
        else:
            status = "severe"
        
        return penalty, status
    
    def compute_total_penalty(self, objectives: Dict[str, float]) -> Dict:
        """è®¡ç®—æ€»æƒ©ç½š"""
        temp = objectives['temp']
        aging = objectives['aging']
        
        temp_penalty, temp_status = self.compute_temperature_penalty(temp)
        aging_penalty, aging_status = self.compute_aging_penalty(aging)
        
        total_penalty = temp_penalty + aging_penalty
        is_severe = (temp_status == "severe" or aging_status == "severe")
        
        return {
            'total_penalty': total_penalty,
            'penalties': {'temp': temp_penalty, 'aging': aging_penalty},
            'statuses': {'temp': temp_status, 'aging': aging_status},
            'is_severe': is_severe
        }


class MultiObjectiveEvaluator:
    """
    å¤šç›®æ ‡å……ç”µç­–ç•¥è¯„ä»·å™¨
    
    åŠŸèƒ½ï¼š
    1. è¯„ä¼°å……ç”µç­–ç•¥çš„ä¸‰ä¸ªç›®æ ‡ï¼ˆæ—¶é—´ã€æ¸©åº¦ã€è€åŒ–ï¼‰
    2. ç»´æŠ¤å†å²æ•°æ®å¹¶åŠ¨æ€æ›´æ–°åˆ†ä½æ•°è¾¹ç•Œ
    3. å½’ä¸€åŒ– + åˆ‡æ¯”é›ªå¤«æ ‡é‡åŒ–
    4. LLM çƒ­å¯åŠ¨æ”¯æŒ
    """
    
    def __init__(
        self, 
        weights: Optional[Dict[str, float]] = None,
        update_interval: int = 10,
        temp_max: float = 309.0,
        max_steps: int = 300,
        verbose: bool = True
    ):
        """
        åˆå§‹åŒ–è¯„ä»·å™¨
        
        å‚æ•°ï¼š
            weights: å„ç›®æ ‡æƒé‡ï¼Œé»˜è®¤ {'time': 0.4, 'temp': 0.35, 'aging': 0.25}
            update_interval: åˆ†ä½æ•°æ›´æ–°é—´éš”ï¼ˆæ¯Næ¬¡è¯„ä¼°æ›´æ–°ä¸€æ¬¡ï¼‰
            temp_max: æ¸©åº¦çº¦æŸä¸Šé™[K]
            max_steps: å•æ¬¡å……ç”µæœ€å¤§æ­¥æ•°é™åˆ¶
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—
        """
        # æƒé‡è®¾ç½®
        self.weights = weights or {
            'time': 0.4,    # å……ç”µæ—¶é—´æƒé‡
            'temp': 0.35,   # å³°å€¼æ¸©åº¦æƒé‡
            'aging': 0.25   # å®¹é‡è€åŒ–æƒé‡
        }
        
        # éªŒè¯æƒé‡å’Œä¸º1
        weight_sum = sum(self.weights.values())
        if not np.isclose(weight_sum, 1.0):
            raise ValueError(f"æƒé‡ä¹‹å’Œå¿…é¡»ä¸º1.0ï¼Œå½“å‰ä¸º {weight_sum}")
        
        # å†å²æ•°æ®å­˜å‚¨
        self.history = {
            'time': [],      # å……ç”µæ­¥æ•°
            'temp': [],      # å³°å€¼æ¸©åº¦[K]
            'aging': [],     # å®¹é‡è¡°å‡[%]
            'valid': []      # æ˜¯å¦æ»¡è¶³çº¦æŸ
        }
        
        # è¯„ä¼°è®¡æ•°
        self.eval_count = 0
        self.update_interval = update_interval
        self.temp_max = temp_max
        self.max_steps = max_steps
        self.verbose = verbose
        
        # åŠ¨æ€åˆ†ä½æ•°è¾¹ç•Œï¼ˆåˆå§‹ä¸ºNoneï¼Œå‰10æ¬¡ç”¨ä¸´æ—¶è¾¹ç•Œï¼‰
        self.bounds = None
        
        # ä¸´æ—¶å›ºå®šè¾¹ç•Œï¼ˆå‰10æ¬¡ä½¿ç”¨ï¼‰
        self.temp_bounds = {
            'time': {'best': 10, 'worst': 150},           # æ­¥æ•°
            'temp': {'best': 298.0, 'worst': temp_max},   # æ¸©åº¦[K]
            'aging': {'best': 0.0, 'worst': 0.5}          # å®¹é‡æŸå¤±[%]
        }
        
        # è¯¦ç»†æ—¥å¿—ï¼ˆç”¨äºåç»­åˆ†æï¼‰
        self.detailed_logs = []
        
        if self.verbose:
            # [Gradient Computation]
            from SPM_v3 import SPM_Sensitivity
            self.spm_for_gradients = SPM_Sensitivity(
                init_v=3.0, 
                init_t=298, 
                mode='finite_difference',
                enable_penalty_gradients=True,
                penalty_scale=10.0
            )
            self.gradient_compute_interval = 5
            print("[OK] Gradient computation enabled with penalty gradients (v3.0)")
            print("=" * 70)
            print("å¤šç›®æ ‡è¯„ä»·å™¨å·²åˆå§‹åŒ–")
            print("=" * 70)
            print(f"æƒé‡è®¾ç½®: {self.weights}")
            print(f"åˆ†ä½æ•°æ›´æ–°é—´éš”: æ¯ {update_interval} æ¬¡è¯„ä¼°")
            print(f"æ¸©åº¦çº¦æŸä¸Šé™: {temp_max} K")
            print(f"æœ€å¤§æ­¥æ•°é™åˆ¶: {max_steps} æ­¥")
            print(f"ä¸´æ—¶è¾¹ç•Œï¼ˆå‰10æ¬¡ï¼‰:")
            print(f"  æ—¶é—´: {self.temp_bounds['time']}")
            print(f"  æ¸©åº¦: {self.temp_bounds['temp']}")
            print(f"  è€åŒ–: {self.temp_bounds['aging']}")
            print("=" * 70)
        
        self.soft_constraints = SoftConstraintHandler(
            temp_max=315.0,
            temp_penalty_rate=0.15,
            temp_penalty_scale=0.05,
            aging_threshold=0.3,
            aging_penalty_scale=0.1,
            verbose=self.verbose
        )
    
    # ============================================================
    # æ–°å¢ï¼šLLM Warm Start æ–¹æ³•
    # ============================================================
    
    async def initialize_with_llm_warmstart(
        self,
        n_strategies: int = 5,
        llm_api_key: Optional[str] = None,
        llm_base_url: str = 'https://api.nuwaapi.com/v1',
        llm_model: str = "gpt-3.5-turbo"
    ) -> List[Dict]:
        """
        ä½¿ç”¨ LLM è¿›è¡Œçƒ­å¯åŠ¨åˆå§‹åŒ– - æ–°ç‰ˆæœ¬ï¼ˆå†å²æ•°æ®é©±åŠ¨ï¼‰
        
        æ”¹è¿›ï¼š
        1. ä½¿ç”¨HistoricalWarmStartè‡ªåŠ¨åŠ è½½å†å²æ•°æ®
        2. åŸºäºç”µåŒ–å­¦é¢†åŸŸçŸ¥è¯†ç”Ÿæˆé«˜è´¨é‡prompt
        3. åŒ…å«few-shot learning examplesï¼ˆæœ€ä¼˜/æœ€å·®è§£ï¼‰
        4. åŠ¨æ€æ¢ç´¢æ¨¡å¼ï¼ˆconservative/balanced/aggressiveï¼‰
        
        å‚æ•°ï¼š
            n_strategies: è¦ç”Ÿæˆçš„ç­–ç•¥æ•°é‡
            llm_api_key: LLM API å¯†é’¥ï¼ˆå¯é€‰ï¼‰
            llm_base_url: API åŸºç¡€ URL
            llm_model: ä½¿ç”¨çš„ LLM æ¨¡å‹
        
        è¿”å›ï¼š
            List[Dict]: è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        if self.verbose:
            print("\n" + "=" * 70)
            print("å¼€å§‹ LLM Warm Start åˆå§‹åŒ–ï¼ˆå†å²æ•°æ®é©±åŠ¨ç‰ˆï¼‰")
            print("=" * 70)
            print(f"ç›®æ ‡ç­–ç•¥æ•°é‡: {n_strategies}")
            print(f"API Key: {'å·²æä¾›' if llm_api_key else 'æœªæä¾›ï¼ˆå°†ä½¿ç”¨éšæœºç­–ç•¥ï¼‰'}")
            print("=" * 70)
        
        results = []
        
        # ====== ä½¿ç”¨æ–°çš„HistoricalWarmStart ======
        if llm_api_key is not None:
            try:
                if self.verbose:
                    print("\nä½¿ç”¨HistoricalWarmStartç”Ÿæˆç­–ç•¥...")
                
                # åˆ›å»ºHistoricalWarmStartå®ä¾‹
                warmstart_generator = HistoricalWarmStart(
                    result_dir='./results',  # ç¡®ä¿Resultç›®å½•æ­£ç¡®
                    llm_api_key=llm_api_key,
                    llm_base_url=llm_base_url,
                    llm_model=llm_model,
                    verbose=self.verbose
                )
                
                # ç”Ÿæˆç­–ç•¥ï¼ˆè‡ªåŠ¨åŠ è½½å†å²æ•°æ® + ç”Ÿæˆé«˜è´¨é‡prompt + è°ƒç”¨LLMï¼‰
                strategies = await warmstart_generator.generate_warmstart_strategies_async(
                    n_strategies=n_strategies,
                    n_historical_runs=5,  # åŠ è½½æœ€è¿‘5æ¬¡è¿è¡Œ
                    objective_weights=self.weights,
                    exploration_mode='balanced'  # å¯é€‰: 'conservative', 'balanced', 'aggressive'
                )
                
                if self.verbose:
                    print(f"[OK] HistoricalWarmStartæˆåŠŸç”Ÿæˆ {len(strategies)} ä¸ªç­–ç•¥")
                
            except Exception as e:
                if self.verbose:
                    print(f"[Warning]  HistoricalWarmStartå¤±è´¥: {e}")
                    print("   å›é€€åˆ°éšæœºç­–ç•¥ç”Ÿæˆ...")
                
                # å›é€€åˆ°éšæœºç­–ç•¥
                strategies = self._generate_random_strategies(n_strategies)
        
        else:
            # æ²¡æœ‰ API keyï¼Œç›´æ¥ä½¿ç”¨éšæœºç­–ç•¥
            if self.verbose:
                print("\nä½¿ç”¨éšæœºç­–ç•¥ç”Ÿæˆ...")
            
            strategies = self._generate_random_strategies(n_strategies)
        
        # ====== è¯„ä¼°æ‰€æœ‰ç­–ç•¥ï¼ˆä¿æŒä¸å˜ï¼‰ ======
        if self.verbose:
            print(f"\nå¼€å§‹è¯„ä¼° {len(strategies)} ä¸ªç­–ç•¥...")
        
        for i, strategy in enumerate(strategies, 1):
            params = strategy['params']
            
            try:
                # è°ƒç”¨ evaluate() æ–¹æ³•è¿›è¡Œä»¿çœŸè¯„ä¼°
                scalarized = self.evaluate(
                    current1=params['current1'],
                    charging_number=params['charging_number'],
                    current2=params['current2']
                )
                
                # ä»æœ€æ–°çš„æ—¥å¿—ä¸­æå–ç›®æ ‡å€¼
                latest_log = self.detailed_logs[-1]
                
                result = {
                    'params': params,
                    'scalarized': scalarized,
                    'objectives': latest_log['objectives'],
                    'valid': latest_log['valid'],
                    'source': strategy.get('source', 'llm_warmstart'),
                    'reasoning': strategy.get('reasoning', '')
                }
                
                results.append(result)
                
                if self.verbose:
                    print(f"  ç­–ç•¥ {i}/{len(strategies)}: "
                          f"I1={params['current1']:.2f}A, "
                          f"t1={params['charging_number']}, "
                          f"I2={params['current2']:.2f}A "
                          f"â†’ æ ‡é‡åŒ–={scalarized:.4f}")
            
            except Exception as e:
                if self.verbose:
                    print(f"  âœ— ç­–ç•¥ {i} è¯„ä¼°å¤±è´¥: {e}")
                continue
        
        if self.verbose:
            print(f"\n[OK] Warm Startå®Œæˆï¼æˆåŠŸè¯„ä¼° {len(results)}/{len(strategies)} ä¸ªç­–ç•¥")
            print("=" * 70)
        
        return results
    
    # [X] å·²åˆ é™¤ _llm_generate_strategies æ–¹æ³•
    # åŸå› : ä½¿ç”¨æ–°çš„HistoricalWarmStartæ›¿ä»£ç¡¬ç¼–ç prompt
    # æ–°æ–¹æ³•è‡ªåŠ¨åŠ è½½å†å²æ•°æ®ï¼Œç”ŸæˆåŸºäºç”µåŒ–å­¦é¢†åŸŸçŸ¥è¯†çš„é«˜è´¨é‡prompt
    
    def _generate_random_strategies(self, n_strategies: int) -> List[Dict]:
        """
        ç”Ÿæˆéšæœºå……ç”µç­–ç•¥ï¼ˆä½œä¸º LLM çš„å›é€€æ–¹æ¡ˆï¼‰
        
        è¿”å›ï¼š
            List[Dict]: éšæœºç­–ç•¥åˆ—è¡¨
        """
        strategies = []
        
        for _ in range(n_strategies):
            strategy = {
                'params': {
                    'current1': np.random.uniform(3.0, 6.0),
                    'charging_number': int(np.random.uniform(5, 25)),
                    'current2': np.random.uniform(1.0, 4.0)
                },
                'source': 'random_warmstart',
                'reasoning': 'Randomly generated strategy'
            }
            strategies.append(strategy)
        
        return strategies
    
    # ============================================================
    # åŸæœ‰æ–¹æ³•ï¼ˆä¿æŒä¸å˜ï¼‰
    # ============================================================
    
    def evaluate(
        self, 
        current1: float, 
        charging_number: float, 
        current2: float
    ) -> float:
        """
        è¯„ä¼°å•æ¬¡å……ç”µç­–ç•¥ï¼ˆä¸»æ¥å£ï¼ŒBOè°ƒç”¨ï¼‰
        
        å‚æ•°ï¼š
            current1: ç¬¬ä¸€é˜¶æ®µå……ç”µç”µæµ[A]
            charging_number: é˜¶æ®µåˆ‡æ¢æ­¥æ•°
            current2: ç¬¬äºŒé˜¶æ®µå……ç”µç”µæµ[A]
        
        è¿”å›ï¼š
            scalarized_value: åˆ‡æ¯”é›ªå¤«æ ‡é‡åŒ–åçš„å€¼ï¼ˆç”¨äºæœ€å°åŒ–ï¼‰
        """
        # 1. è¿è¡Œå……ç”µä»¿çœŸ
        sim_result = self._run_charging_simulation(current1, charging_number, current2)
        
        # 2. æ›´æ–°å†å²
        self.history['time'].append(sim_result['time'])
        self.history['temp'].append(sim_result['temp'])
        self.history['aging'].append(sim_result['aging'])
        self.history['valid'].append(sim_result['valid'])
        
        self.eval_count += 1
        
        # 3. æ¯Næ¬¡æ›´æ–°åˆ†ä½æ•°è¾¹ç•Œ
        if self.eval_count % self.update_interval == 0 and self.eval_count >= 10:
            self._update_bounds()
        
        # 4. å½’ä¸€åŒ–
        objectives_only = {
            'time': sim_result['time'],
            'temp': sim_result['temp'],
            'aging': sim_result['aging']
        }
        normalized = self._normalize(objectives_only)
        
        # 5. åˆ‡æ¯”é›ªå¤«æ ‡é‡åŒ–
        scalarized = self._chebyshev_scalarize(normalized)
        
        # 6. è½¯çº¦æŸæƒ©ç½šæœºåˆ¶ (æŒ‡æ•°/å¹³æ–¹)
        constraint_result = self.soft_constraints.compute_total_penalty(objectives_only)
        soft_penalty = constraint_result['total_penalty']
        scalarized += soft_penalty
        
        if constraint_result['is_severe']:
            scalarized += 0.1
        
        # 6.5 è®¡ç®—æ¢¯åº¦
        gradients = None
        if self.eval_count % 3 == 0:
            try:
                grad_result = self.spm_for_gradients.run_two_stage_charging(
                    current1=current1, charging_number=int(charging_number), 
                    current2=current2, return_sensitivities=True
                )
                if grad_result['valid'] and 'sensitivities' in grad_result:
                    gradients = grad_result['sensitivities']
            except:
                gradients = None


        # 7. è®°å½•è¯¦ç»†æ—¥å¿—
        log_entry = {
            'eval_id': self.eval_count,
            'params': {'current1': current1, 'charging_number': charging_number, 'current2': current2},
            'objectives': objectives_only,
            'normalized': normalized,
            'scalarized': scalarized,
            'valid': sim_result['valid'],
            'violations': sim_result['constraint_violation'],
            'termination': sim_result['termination'],
            'gradients': gradients
        }
        self.detailed_logs.append(log_entry)
        
        # 8. å¯é€‰ï¼šæ‰“å°è¿›åº¦
        if self.verbose and self.eval_count % 5 == 0:
            time_minutes = sim_result['time'] * 90 / 60
            
            constraint_info = self.soft_constraints.compute_total_penalty(objectives_only)
            temp_status = constraint_info['statuses']['temp']
            temp_penalty = constraint_info['penalties']['temp']
            
            status_icon = {
                'safe': 'âœ“',
                'mild': 'âš ',
                'moderate': 'âš âš ',
                'severe': 'âŒ'
            }.get(temp_status, '?')
            
            print(f"[Eval {self.eval_count}] "
                  f"t={sim_result['time']:.0f}æ­¥({time_minutes:.1f}min), "
                  f"T={sim_result['temp']:.2f}K{status_icon}, "
                  f"A={sim_result['aging']:.4f}%, "
                  f"penalty={temp_penalty:.4f}, "
                  f"f={scalarized:.4f}")
        
        return scalarized
    
    def _run_charging_simulation(
        self, 
        current1: float, 
        charging_number: float, 
        current2: float
    ) -> Dict:
        """è¿è¡Œå……ç”µä»¿çœŸå¹¶æ”¶é›†ä¸‰ä¸ªç›®æ ‡"""
        # åˆå§‹åŒ–SPMç¯å¢ƒï¼ˆv3.0 - ä¸éœ€è¦çµæ•åº¦ï¼‰
        env = SPM(init_v=3.0, init_t=298)
        
        # è¿è¡Œä¸¤é˜¶æ®µå……ç”µä»¿çœŸ
        result = env.run_two_stage_charging(
            current1=current1,
            charging_number=int(charging_number),
            current2=current2,
            return_sensitivities=False
        )
        
        if not result['valid']:
            return {
                'time': self.max_steps,  # æƒ©ç½šå€¼ï¼ˆæ­¥æ•°ï¼‰
                'temp': self.temp_max + 10,  # æƒ©ç½šå€¼
                'aging': 1.0,  # æƒ©ç½šå€¼
                'valid': False,
                'constraint_violation': 1,
                'termination': 'invalid'
            }
        
        # è½¬æ¢ç»“æœæ ¼å¼ï¼šå°†æ—¶é—´ä»ç§’è½¬æ¢ä¸ºæ­¥æ•°ï¼ˆ1æ­¥ = 90ç§’ï¼‰
        objectives = result['objectives']
        time_in_steps = objectives['time'] / 90.0  # ç§’ -> æ­¥æ•°
        
        return {
            'time': time_in_steps,  # æ­¥æ•°
            'temp': objectives['temp'],  # K
            'aging': objectives['aging'],  # %
            'valid': result['valid'],
            'constraint_violation': 0,
            'termination': 'completed'
        }
    
    def _update_bounds(self) -> None:
        """
        æ›´æ–°åˆ†ä½æ•°è¾¹ç•Œï¼ˆQ5/Q95ï¼‰
        
        æ”¹è¿›: ç¡®ä¿è¾¹ç•Œæœ‰åˆç†çš„æœ€å°èŒƒå›´,é¿å…æ•°å€¼ä¸ç¨³å®š
        """
        if self.eval_count < 10:
            return
        
        valid_indices = [i for i, v in enumerate(self.history['valid']) if v]
        
        if len(valid_indices) < 5:
            return
        
        # è®¡ç®—Q5/Q95åˆ†ä½æ•°
        self.bounds = {
            'time': {
                'best': np.percentile([self.history['time'][i] for i in valid_indices], 5),
                'worst': np.percentile([self.history['time'][i] for i in valid_indices], 95)
            },
            'temp': {
                'best': np.percentile([self.history['temp'][i] for i in valid_indices], 5),
                'worst': np.percentile([self.history['temp'][i] for i in valid_indices], 95)
            },
            'aging': {
                'best': np.percentile([self.history['aging'][i] for i in valid_indices], 5),
                'worst': np.percentile([self.history['aging'][i] for i in valid_indices], 95)
            }
        }
        
        # âœ… ç¡®ä¿è¾¹ç•Œæœ‰æœ€å°èŒƒå›´ï¼ˆé¿å…é™¤é›¶ï¼‰
        min_ranges = {
            'time': 5.0,      # æœ€å°æ—¶é—´èŒƒå›´5æ­¥
            'temp': 1.0,      # æœ€å°æ¸©åº¦èŒƒå›´1K
            'aging': 0.01     # æœ€å°è€åŒ–èŒƒå›´0.01%
        }
        
        for key, min_range in min_ranges.items():
            current_range = self.bounds[key]['worst'] - self.bounds[key]['best']
            if current_range < min_range:
                # æ‰©å±•è¾¹ç•Œåˆ°æœ€å°èŒƒå›´
                mid = (self.bounds[key]['best'] + self.bounds[key]['worst']) / 2
                self.bounds[key]['best'] = mid - min_range / 2
                self.bounds[key]['worst'] = mid + min_range / 2
                
                if self.verbose:
                    print(f"  [è¾¹ç•Œè°ƒæ•´] {key} èŒƒå›´è¿‡çª„ ({current_range:.4f}), æ‰©å±•åˆ° {min_range}")
        
        if self.verbose:
            print(f"\n[åˆ†ä½æ•°è¾¹ç•Œå·²æ›´æ–°] (ç¬¬ {self.eval_count} æ¬¡è¯„ä¼°)")
            for key in ['time', 'temp', 'aging']:
                print(f"  {key}: [{self.bounds[key]['best']:.4f}, {self.bounds[key]['worst']:.4f}]")
    
    def _normalize(self, objectives: Dict[str, float]) -> Dict[str, float]:
        """
        å½’ä¸€åŒ–ç›®æ ‡å€¼åˆ° [0, 1]
        
        æ”¹è¿›: æ·»åŠ æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤,é¿å…è¾¹ç•Œè¿‡çª„å¯¼è‡´é™¤é›¶é”™è¯¯
        """
        # ä¼˜å…ˆä½¿ç”¨åŠ¨æ€è¾¹ç•Œ,ä¸è¶³åˆ™ä½¿ç”¨ä¸´æ—¶è¾¹ç•Œ
        bounds = self.bounds if self.bounds is not None else self.temp_bounds
        
        # âœ… å®šä¹‰æœ€å°æœ‰æ•ˆèŒƒå›´ï¼ˆç›¸å¯¹äºä¸´æ—¶è¾¹ç•Œï¼‰
        min_ranges = {
            'time': 5.0,      # æœ€å°æ—¶é—´èŒƒå›´5æ­¥ï¼ˆä¸´æ—¶è¾¹ç•ŒèŒƒå›´140ï¼‰
            'temp': 1.0,      # æœ€å°æ¸©åº¦èŒƒå›´1Kï¼ˆä¸´æ—¶è¾¹ç•ŒèŒƒå›´11Kï¼‰
            'aging': 0.001    # æœ€å°è€åŒ–èŒƒå›´0.001%ï¼ˆä¸´æ—¶è¾¹ç•ŒèŒƒå›´0.5%ï¼‰
        }
        
        normalized = {}
        for key in ['time', 'temp', 'aging']:
            best = bounds[key]['best']
            worst = bounds[key]['worst']
            value = objectives[key]
            
            # è®¡ç®—åˆ†æ¯
            denominator = worst - best
            min_range = min_ranges[key]
            
            # âœ… æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼šè¾¹ç•ŒèŒƒå›´å¿…é¡»å¤§äºæœ€å°æœ‰æ•ˆèŒƒå›´
            if abs(denominator) < min_range:
                # è¾¹ç•Œè¿‡çª„,å°è¯•ä½¿ç”¨ä¸´æ—¶è¾¹ç•Œä½œä¸ºfallback
                if bounds is not self.temp_bounds:
                    # å½“å‰ä½¿ç”¨çš„æ˜¯åŠ¨æ€è¾¹ç•Œ,fallbackåˆ°ä¸´æ—¶è¾¹ç•Œ
                    temp_best = self.temp_bounds[key]['best']
                    temp_worst = self.temp_bounds[key]['worst']
                    temp_denominator = temp_worst - temp_best
                    
                    if abs(temp_denominator) >= min_range:
                        # ä¸´æ—¶è¾¹ç•Œæœ‰æ•ˆ
                        normalized[key] = (value - temp_best) / temp_denominator
                        if self.verbose and self.eval_count % 10 == 0:
                            print(f"  [Warning] {key}åŠ¨æ€è¾¹ç•Œè¿‡çª„({denominator:.6f}<{min_range}), ä½¿ç”¨å›ºå®šè¾¹ç•Œ")
                    else:
                        # ä¸´æ—¶è¾¹ç•Œä¹Ÿè¿‡çª„,ä½¿ç”¨ä¸­é—´å€¼
                        normalized[key] = 0.5
                        if self.verbose and self.eval_count % 10 == 0:
                            print(f"  [Warning] {key}æ‰€æœ‰è¾¹ç•Œè¿‡çª„, ä½¿ç”¨é»˜è®¤å€¼0.5")
                else:
                    # å·²ç»åœ¨ä½¿ç”¨ä¸´æ—¶è¾¹ç•Œä¸”ä»è¿‡çª„
                    normalized[key] = 0.5
                    if self.verbose and self.eval_count % 10 == 0:
                        print(f"  [Warning] {key}å›ºå®šè¾¹ç•Œè¿‡çª„({denominator:.6f}<{min_range}), ä½¿ç”¨é»˜è®¤å€¼0.5")
            else:
                # è¾¹ç•Œæ­£å¸¸,è¿›è¡Œæ ‡å‡†å½’ä¸€åŒ–
                normalized[key] = (value - best) / denominator
            
            # è£å‰ªåˆ°[0, 1]èŒƒå›´
            normalized[key] = np.clip(normalized[key], 0.0, 1.0)
        
        return normalized
    
    def _chebyshev_scalarize(self, normalized: Dict[str, float]) -> float:
        """å¢å¼ºåˆ‡æ¯”é›ªå¤«æ ‡é‡åŒ–"""
        weighted_deviations = [
            self.weights[key] * normalized[key] 
            for key in ['time', 'temp', 'aging']
        ]
        
        max_weighted = max(weighted_deviations)
        sum_weighted = sum(weighted_deviations)
        
        rho = 0.05
        scalarized = max_weighted + rho * sum_weighted
        
        return scalarized
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_evaluations': self.eval_count,
            'valid_evaluations': sum(self.history['valid']),
            'history_summary': {}
        }
        
        for key in ['time', 'temp', 'aging']:
            if len(self.history[key]) > 0:
                stats['history_summary'][key] = {
                    'min': float(np.min(self.history[key])),
                    'max': float(np.max(self.history[key])),
                    'mean': float(np.mean(self.history[key])),
                    'std': float(np.std(self.history[key]))
                }
        
        return stats
    
    def export_database(self) -> List[Dict]:
        """å¯¼å‡ºå®Œæ•´æ•°æ®åº“"""
        return self.detailed_logs
    
    def get_pareto_front(self) -> List[Dict]:
        """æå–å¸•ç´¯æ‰˜æœ€ä¼˜è§£"""
        valid_logs = [log for log in self.detailed_logs if log['valid']]
        
        if len(valid_logs) == 0:
            return []
        
        pareto_front = []
        
        for log_i in valid_logs:
            obj_i = log_i['objectives']
            is_dominated = False
            
            for log_j in valid_logs:
                if log_i == log_j:
                    continue
                
                obj_j = log_j['objectives']
                
                # æ£€æŸ¥æ˜¯å¦è¢«æ”¯é…
                dominates = (
                    obj_j['time'] <= obj_i['time'] and
                    obj_j['temp'] <= obj_i['temp'] and
                    obj_j['aging'] <= obj_i['aging']
                )
                
                at_least_one_better = (
                    obj_j['time'] < obj_i['time'] or
                    obj_j['temp'] < obj_i['temp'] or
                    obj_j['aging'] < obj_i['aging']
                )
                
                if dominates and at_least_one_better:
                    is_dominated = True
                    break
            
            if not is_dominated:
                pareto_front.append(log_i)
        
        return pareto_front
    
    def add_external_evaluation(
        self, 
        current1: float, 
        charging_number: float, 
        current2: float,
        objectives: Dict[str, float],
        source: str = "external"
    ) -> None:
        """æ‰‹åŠ¨æ·»åŠ å¤–éƒ¨è¯„ä¼°ç»“æœåˆ°æ•°æ®åº“"""
        self.history['time'].append(objectives['time'])
        self.history['temp'].append(objectives['temp'])
        self.history['aging'].append(objectives['aging'])
        self.history['valid'].append(objectives.get('valid', True))
        
        self.eval_count += 1
        
        normalized = self._normalize(objectives)
        scalarized = self._chebyshev_scalarize(normalized)
        
        log_entry = {
            'eval_id': self.eval_count,
            'params': {'current1': current1, 'charging_number': charging_number, 'current2': current2},
            'objectives': objectives,
            'normalized': normalized,
            'scalarized': scalarized,
            'valid': objectives.get('valid', True),
            'violations': 0,
            'termination': 'external',
            'source': source
        }
        self.detailed_logs.append(log_entry)
        
        if self.verbose:
            print(f"[å¤–éƒ¨è¯„ä¼°å·²æ·»åŠ ] æ¥æº={source}, è¯„ä¼°ID={self.eval_count}")
    
    def update_weights(self, new_weights: Dict[str, float]) -> None:
        """åŠ¨æ€æ›´æ–°ç›®æ ‡æƒé‡"""
        weight_sum = sum(new_weights.values())
        if not np.isclose(weight_sum, 1.0):
            raise ValueError(f"æƒé‡ä¹‹å’Œå¿…é¡»ä¸º1.0ï¼Œå½“å‰ä¸º {weight_sum}")
        
        self.weights = new_weights
        
        if self.verbose:
            print(f"[æƒé‡å·²æ›´æ–°] {self.weights}")
    
    def get_best_solution(self) -> Optional[Dict]:
        """è·å–å½“å‰æœ€ä½³è§£ï¼ˆåŸºäºæ ‡é‡åŒ–å€¼ï¼‰"""
        if self.eval_count == 0:
            return None
        
        valid_logs = [log for log in self.detailed_logs if log['valid']]
        
        if len(valid_logs) == 0:
            return None
        
        best_log = min(valid_logs, key=lambda x: x['scalarized'])
        
        return best_log


# ============================================================
# æµ‹è¯•ä»£ç 
# ============================================================
if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("ã€æµ‹è¯•ã€‘MultiObjectiveEvaluator - v3.1ï¼ˆå·²ä¿®å¤ï¼‰")
    print("=" * 70)
    
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    evaluator = MultiObjectiveEvaluator(verbose=True)
    
    print("\n[OK] Basic functionality test passed")
    print(f"   evaluate() æ–¹æ³•: å­˜åœ¨")
    print(f"   initialize_with_llm_warmstart() æ–¹æ³•: å­˜åœ¨ âœ¨")
    print(f"   get_best_solution() æ–¹æ³•: å­˜åœ¨")
    print(f"   get_pareto_front() æ–¹æ³•: å­˜åœ¨")
    print("=" * 70)