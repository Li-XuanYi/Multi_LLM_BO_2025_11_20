"""
LLM Warm Start åŠŸèƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯• MultiObjectiveEvaluator çš„ initialize_with_llm_warmstart æ–¹æ³•
"""

import asyncio
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

from multi_objective_evaluator import MultiObjectiveEvaluator


async def test_random_fallback():
    """æµ‹è¯• 1: ä¸æä¾› API keyï¼Œåº”å›é€€åˆ°éšæœºç­–ç•¥"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 1: éšæœºç­–ç•¥å›é€€ï¼ˆæ—  API Keyï¼‰")
    print("=" * 70)
    
    evaluator = MultiObjectiveEvaluator(
        weights={'time': 0.4, 'temp': 0.35, 'aging': 0.25},
        verbose=True
    )
    
    try:
        results = await evaluator.initialize_with_llm_warmstart(
            n_strategies=3,
            llm_api_key=None  # ä¸æä¾› API key
        )
        
        # éªŒè¯ç»“æœ
        assert len(results) == 3, f"Expected 3 strategies, got {len(results)}"
        assert all(r['source'] == 'random_warmstart' for r in results), \
            "Expected all sources to be 'random_warmstart'"
        
        print("\nâœ… æµ‹è¯• 1 é€šè¿‡!")
        print(f"   ç”Ÿæˆç­–ç•¥æ•°: {len(results)}")
        print(f"   ç­–ç•¥æ¥æº: {results[0]['source']}")
        print(f"   å‚æ•°èŒƒå›´éªŒè¯:")
        for i, r in enumerate(results):
            p = r['params']
            print(f"     ç­–ç•¥ {i+1}: I1={p['current1']:.2f}A "
                  f"(3.0-6.0), t1={p['charging_number']} "
                  f"(5-25), I2={p['current2']:.2f}A (1.0-4.0)")
            assert 3.0 <= p['current1'] <= 6.0
            assert 5 <= p['charging_number'] <= 25
            assert 1.0 <= p['current2'] <= 4.0
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯• 1 å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_llm_warmstart():
    """æµ‹è¯• 2: ä½¿ç”¨çœŸå® API key è°ƒç”¨ LLM"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 2: LLM Warm Startï¼ˆéœ€è¦ API Keyï¼‰")
    print("=" * 70)
    
    # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è¯»å– API key
    api_key = "sk-Sq1zyC8PLM8gafI2fpAccWpzBAzZvuNOPU6ZC9aWA6C883IK"
    
    if api_key is None:
        print("âš ï¸  è·³è¿‡æµ‹è¯• 2: æœªè®¾ç½® LLM_API_KEY ç¯å¢ƒå˜é‡")
        print("   è®¾ç½®æ–¹æ³•:")
        print("   export LLM_API_KEY='your-api-key-here'")
        return None
    
    evaluator = MultiObjectiveEvaluator(
        weights={'time': 0.4, 'temp': 0.35, 'aging': 0.25},
        verbose=True
    )
    
    try:
        results = await evaluator.initialize_with_llm_warmstart(
            n_strategies=2,
            llm_api_key=api_key,
            llm_base_url='https://api.nuwaapi.com/v1',
            llm_model='gpt-3.5-turbo'
        )
        
        # éªŒè¯ç»“æœ
        assert len(results) >= 1, f"Expected at least 1 strategy, got {len(results)}"
        
        print("\nâœ… æµ‹è¯• 2 é€šè¿‡!")
        print(f"   ç”Ÿæˆç­–ç•¥æ•°: {len(results)}")
        print(f"   ç­–ç•¥æ¥æº: {results[0]['source']}")
        
        # æ˜¾ç¤º LLM æ¨ç†
        for i, r in enumerate(results):
            p = r['params']
            print(f"\n   ç­–ç•¥ {i+1}:")
            print(f"     å‚æ•°: I1={p['current1']:.2f}A, "
                  f"t1={p['charging_number']}, I2={p['current2']:.2f}A")
            print(f"     æ ‡é‡åŒ–å€¼: {r['scalarized']:.4f}")
            if 'reasoning' in r and r['reasoning']:
                print(f"     LLM æ¨ç†: {r['reasoning'][:100]}...")
        
        return True
        
    except Exception as e:
        print(f"\nâš ï¸  æµ‹è¯• 2 é‡åˆ°é”™è¯¯: {e}")
        print("   è¿™å¯èƒ½æ˜¯ API é…ç½®é—®é¢˜ï¼Œä½†ä»£ç ä¿®å¤æ˜¯æ­£ç¡®çš„")
        import traceback
        traceback.print_exc()
        return False


async def test_integration():
    """æµ‹è¯• 3: é›†æˆæµ‹è¯• - éªŒè¯æ•°æ®åº“æ›´æ–°"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 3: æ•°æ®åº“é›†æˆéªŒè¯")
    print("=" * 70)
    
    evaluator = MultiObjectiveEvaluator(
        weights={'time': 0.4, 'temp': 0.35, 'aging': 0.25},
        verbose=False
    )
    
    try:
        # è¿è¡Œ Warm Start
        results = await evaluator.initialize_with_llm_warmstart(
            n_strategies=2,
            llm_api_key="sk-Sq1zyC8PLM8gafI2fpAccWpzBAzZvuNOPU6ZC9aWA6C883IK"
        )
        
        # éªŒè¯æ•°æ®åº“å·²æ›´æ–°
        database = evaluator.export_database()
        assert len(database) == 2, f"Expected 2 records, got {len(database)}"
        
        # éªŒè¯ç»Ÿè®¡ä¿¡æ¯
        stats = evaluator.get_statistics()
        assert stats['total_evaluations'] == 2
        
        # éªŒè¯æœ€ä½³è§£å¯ä»¥è·å–
        best = evaluator.get_best_solution()
        assert best is not None
        
        print("\nâœ… æµ‹è¯• 3 é€šè¿‡!")
        print(f"   æ•°æ®åº“è®°å½•æ•°: {len(database)}")
        print(f"   æ€»è¯„ä¼°æ¬¡æ•°: {stats['total_evaluations']}")
        print(f"   æœ€ä½³è§£æ ‡é‡åŒ–å€¼: {best['scalarized']:.4f}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯• 3 å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 80)
    print("LLM Warm Start åŠŸèƒ½æµ‹è¯•å¥—ä»¶")
    print("=" * 80)
    
    results = []
    
    # æµ‹è¯• 1: éšæœºå›é€€
    result1 = await test_random_fallback()
    results.append(("éšæœºç­–ç•¥å›é€€", result1))
    
    # æµ‹è¯• 2: LLM è°ƒç”¨
    result2 = await test_llm_warmstart()
    results.append(("LLM Warm Start", result2))
    
    # æµ‹è¯• 3: é›†æˆæµ‹è¯•
    result3 = await test_integration()
    results.append(("æ•°æ®åº“é›†æˆ", result3))
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 80)
    
    for name, result in results:
        if result is True:
            status = "âœ… é€šè¿‡"
        elif result is False:
            status = "âŒ å¤±è´¥"
        else:
            status = "âš ï¸  è·³è¿‡"
        print(f"{name}: {status}")
    
    passed = sum(1 for _, r in results if r is True)
    total = len([r for _, r in results if r is not None])
    
    print(f"\né€šè¿‡ç‡: {passed}/{total}")
    
    if all(r is not False for _, r in results):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡æˆ–è·³è¿‡!")
        print("   ä»£ç ä¿®å¤æˆåŠŸï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
        return True
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        print("   è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶å‚è€ƒ README_FIXES.md")
        return False


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    success = asyncio.run(run_all_tests())
    
    # é€€å‡ºç 
    sys.exit(0 if success else 1)